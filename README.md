# MakeIt3D - Backend-for-Frontend (BFF)

## Overview

This is the Backend-for-Frontend (BFF) service for the MakeIt3D application. It acts as a secure and streamlined intermediary between the MakeIt3D mobile frontend and external AI services, specifically Tripo AI (for 3D model generation) and OpenAI (for 2D concept generation).

The primary responsibilities of this BFF are:
-   Securely managing API keys for external services.
-   Abstracting the complexities of external AI APIs.
-   Providing a tailored and simplified API for the mobile frontend.
-   Handling asynchronous task management for AI generation processes using Celery and Redis.
-   Implementing rate limiting to ensure fair usage and protect backend services.

The mobile frontend (detailed in `.documentation/frontend_architecture.md`) communicates with this BFF to initiate 2D and 3D asset generation, poll for task status, and receive temporary URLs for the generated assets. The frontend is then responsible for downloading these assets and managing their storage (e.g., uploading to Supabase).

## Key Features

-   **Unified API for AI Generation**: Provides endpoints for:
    -   Image-to-Image (2D concepts via OpenAI)
    -   Text-to-Model (3D models via Tripo AI)
    -   Image-to-Model (3D models via Tripo AI)
    -   Sketch-to-Model (3D models via Tripo AI)
    -   Photo-to-Model (3D models via Tripo AI)
    -   Model Refinement (via Tripo AI)
-   **Task Status Polling**: Allows the frontend to check the status of ongoing generation tasks.
-   **Secure API Key Management**: External API keys are stored and managed securely on the BFF server.
-   **Prompt Customization**: Adapts user prompts and style selections for optimal results from AI services.
-   **Asynchronous Task Processing**: Leverages Celery workers to handle long-running AI generation tasks without blocking API responses.
-   **Rate Limiting**: Implements per-client rate limits on API endpoints using `fastapi-limiter` and `slowapi`.

## Technology Stack

-   **Language**: Python 3.x
-   **Framework**: FastAPI
-   **Asynchronous HTTP Client**: httpx (for calling external AI APIs)
-   **Data Validation & Settings**: Pydantic
-   **Task Queue**: Celery with Redis as the broker and result backend
-   **Containerization**: Docker
-   **API Documentation**: OpenAPI (auto-generated by FastAPI)

For a detailed architectural overview, please refer to `.documentation/bff_architecture.md`.

## Setup and Running

The application is designed to be run using Docker and Docker Compose.

1.  **Environment Variables**:
    Ensure you have a `.env` file at the root of the project with the necessary API keys and configurations (refer to `docker-compose.yml` for required variables like `TRIPO_API_KEY`, `OPENAI_API_KEY`, `REDIS_URL`, etc.).

    **⚠️ SECURITY NOTE**: Never commit your `.env` file to version control. It contains sensitive API keys and secrets. The `.env` file is already properly excluded in `.gitignore`.

    Required variables include:
    - `TRIPO_API_KEY` - Your Tripo AI API key
    - `OPENAI_API_KEY` - Your OpenAI API key  
    - `SUPABASE_URL` - Your Supabase project URL
    - `SUPABASE_SERVICE_KEY` - Your Supabase service role key
    - `SUPABASE_ANON_KEY` - Your Supabase anonymous key
    - `REDIS_URL` - Redis connection URL
    - Other variables as needed from `docker-compose.yml`

2.  **Build and Run with Docker Compose**:
    ```bash
    docker-compose up --build
    ```
    This command will build the Docker images for the backend and Celery workers and start all services defined in `docker-compose.yml`.

    The FastAPI application will typically be available at `http://localhost:8000`.
    API documentation (Swagger UI) will be at `http://localhost:8000/docs`.
    Alternative API documentation (ReDoc) will be at `http://localhost:8000/redoc`.

## API Endpoints

The BFF exposes several endpoints primarily under the `/generate/` and `/tasks/` paths. For a detailed list and specifications, please see the [API Endpoints section in the BFF Architecture document](.documentation/bff_architecture.md#api-endpoints) or explore the live API documentation once the service is running.

## Celery Workers

The `docker-compose.yml` file defines several Celery worker services to handle different types of tasks and concurrency requirements:
-   `celery_worker_default`: For general tasks.
-   `celery_worker_tripo_other`: Specifically for most Tripo AI generation tasks, with a higher concurrency.
-   `celery_worker_tripo_refine`: For Tripo AI model refinement tasks, with a dedicated concurrency.

These workers listen to specific queues and process tasks asynchronously, allowing the main FastAPI application to remain responsive.

## Interacting with the Frontend

This BFF is designed to work in conjunction with the MakeIt3D mobile application. The frontend handles user interaction, input gathering, and the subsequent downloading and storage of assets generated via this BFF. Refer to `.documentation/frontend_architecture.md` for details on the frontend application.

## Deployment on Railway

Deploying this BFF to Railway involves the following general steps:

1.  **Push to Git Repository**: Ensure your project, including the `Dockerfile` and `requirements.txt`, is pushed to a GitHub (or similar) repository.
2.  **Create a New Project on Railway**:
    *   Connect Railway to your Git repository.
    *   Railway will typically detect the `Dockerfile` and attempt to build and deploy it.
3.  **Configure Services**:
    *   **Backend (FastAPI)**: Railway should create a service for your main application. Ensure its start command is similar to what's in your `Dockerfile` or `docker-compose.yml` for the `backend` service (e.g., `uvicorn app.main:app --host 0.0.0.0 --port $PORT`). Railway provides a `$PORT` environment variable that your application should bind to.
    *   **Redis**: Add a Redis service from the Railway marketplace.
    *   **Celery Workers**: You will need to create separate services for each Celery worker type (`celery_worker_default`, `celery_worker_tripo_other`, `celery_worker_tripo_refine`).
        *   Each worker service will use the same Docker image built from your repository.
        *   Set the "Start Command" for each worker service according to its command in `docker-compose.yml` (e.g., `celery -A app.celery_worker worker -l info -P eventlet -c 1 -Q default` for the default worker).
4.  **Environment Variables**:
    *   In your Railway project settings (and for each service if necessary), configure all the required environment variables:
        *   `TRIPO_API_KEY`
        *   `OPENAI_API_KEY`
        *   `REDIS_URL`: This will be provided by the Railway Redis service. Update your application to use this URL.
        *   `DATABASE_URL`, `SUPABASE_URL`, `SUPABASE_SERVICE_KEY` (if used directly by BFF, otherwise only for frontend)
        *   `BFF_BASE_URL`: Set this to the public URL of your deployed backend service on Railway.
    *   Ensure Celery worker services also have access to these environment variables, especially `REDIS_URL`, `TRIPO_API_KEY`, and `OPENAI_API_KEY`.
5.  **Networking**:
    *   The backend service will be exposed publicly by Railway.
    *   Redis and Celery workers typically do not need to be exposed publicly and will communicate internally.
6.  **Deploy**: Trigger a deployment in Railway. Monitor the build and deployment logs for any errors.

**Note**: You might need to adjust Python dependencies (e.g., `psycopg2-binary` if using PostgreSQL directly, though your current setup uses Supabase via API/SDKs which is fine) or system packages in your `Dockerfile` based on Railway's build environment if you encounter issues. The `CMD` in your `Dockerfile` will likely be used for the main web service; Celery worker commands will need to be overridden in their respective Railway service configurations. 